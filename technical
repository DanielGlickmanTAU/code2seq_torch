was getting nan values in forward.
Fixes:
1) registered torch hook to find what module was causing the problem
2) manually zeroed out nan values after softmax

MultiheadAttention#forward : line 5101: attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
att_output_weights shape is #nhead,n,n....attn_output_weights.sum(dim=-1) sums aprox 1(dropout)

issue:training with transofrmer layer initalize performed way worse
fix: lower learning rate(saw that training loss was going up and down)

installing ogb manually:
1) install torch 1.10.1
    on cpu:pip3 install torch torchvision torchaudio
    cuda 11.3: pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
    cuda 10.2:pip3 install torch==1.10.1+cu102 torchvision==0.11.2+cu102 torchaudio===0.10.1+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html

2) pytorch geometric
replace ${CUPA} with cpu, cu102, or cu113 depending on your PyTorch installation (torch.version.cuda).
pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.1+${CUDA}.html
pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.1+${CUDA}.html
pip install torch-geometric

3) ogb: pip install ogb>=1.3.2


