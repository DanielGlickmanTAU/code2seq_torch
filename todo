

todo:
-filter bad example before computing __len__ datasets
-print len dataset
-remove if num_nones > len(paths) * 0.5:
- if performance drops for larger vocab sizes, it will be relaxing
- change training patience
- make token_embedding, label_embedding sizes constant, so model sizes are constant


solved:
2/1 terminal with delimnating symbols(e.g \t), cause problems when reading the dataset(pytorch dataloader) line by line
fix: ast_to_graph#convert -> .strip() values..

